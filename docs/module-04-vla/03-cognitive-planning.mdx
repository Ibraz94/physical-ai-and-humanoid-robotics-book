---
title: Chapter 3 - Cognitive Planning with Large Language Models
sidebar_position: 3
slug: /module-04/cognitive-planning
---

  

### **From Text to Actions**

  

Speech recognition gives us text: "Go to the kitchen and bring me a glass of water."

  

Now we need to:

1. Understand the intent

2. Break down into subtasks

3. Generate robot-executable actions

4. Handle failures and replanning

  

This is where LLMs excel.

  

### **Setting Up LLM Integration**

  

We'll use OpenAI's API (GPT-4). You can also use:

- Anthropic Claude

- Google Gemini

- Open-source models (Llama, Mistral)

  

```bash

pip  install  openai

```

  

Create API key at [platform.openai.com](https://platform.openai.com)

  

```python

import openai

  

openai.api_key = "your-api-key-here"

```

  

### **Task Planning Architecture**

  

Create a ROS 2 node that translates commands to actions:

  

```python

import rclpy

from rclpy.node import Node

from std_msgs.msg import String

from geometry_msgs.msg import PoseStamped

import openai

import json

  

class  TaskPlannerNode(Node):

def  __init__(self):

super().__init__('task_planner_node')

# Subscribe to voice commands

self.command_sub = self.create_subscription(

String,

'/voice_command',

self.command_callback,

10

)

# Publish navigation goals

self.nav_goal_pub = self.create_publisher(

PoseStamped,

'/goal_pose',

10

)

# Publish action commands

self.action_pub = self.create_publisher(

String,

'/robot_action',

10

)

# Define available actions

self.available_actions = {

"navigate": "Navigate to a location (requires: location)",

"grasp": "Grasp an object (requires: object_name)",

"place": "Place held object (requires: location)",

"scan": "Scan environment for objects",

"report": "Report findings to user"

}

# Known locations

self.known_locations = {

"kitchen": {"x": 5.0, "y": 2.0},

"living_room": {"x": -3.0, "y": 4.0},

"bedroom": {"x": 8.0, "y": -2.0},

"home": {"x": 0.0, "y": 0.0}

}

openai.api_key = "your-api-key"

def  command_callback(self, msg):

command = msg.data

self.get_logger().info(f'Received command: {command}')

# Generate action plan

plan = self.generate_plan(command)

# Execute plan

self.execute_plan(plan)

def  generate_plan(self, command):

prompt = f"""You are a robot task planner. Convert the user command into a sequence of robot actions.

  

Available actions:

{json.dumps(self.available_actions, indent=2)}

  

Known locations:

{json.dumps(list(self.known_locations.keys()), indent=2)}

  

User command: "{command}"

  

Generate a JSON action plan with this format:

{{

"plan": [

{{"action": "navigate", "parameters": {{"location": "kitchen"}}}},

{{"action": "grasp", "parameters": {{"object": "cup"}}}},

...

]

}}

  

Respond ONLY with valid JSON, no other text."""

  

response = openai.ChatCompletion.create(

model="gpt-4",

messages=[

{"role": "system", "content": "You are a helpful robot task planner."},

{"role": "user", "content": prompt}

],

temperature=0.3

)

plan_text = response.choices[0].message.content.strip()

# Parse JSON

try:

plan = json.loads(plan_text)

self.get_logger().info(f'Generated plan: {json.dumps(plan, indent=2)}')

return plan

except json.JSONDecodeError:

self.get_logger().error(f'Failed to parse plan: {plan_text}')

return {"plan": []}

def  execute_plan(self, plan):

for step in plan.get("plan", []):

action = step.get("action")

params = step.get("parameters", {})

self.get_logger().info(f'Executing: {action} with {params}')

if action == "navigate":

self.execute_navigate(params.get("location"))

elif action == "grasp":

self.execute_grasp(params.get("object"))

elif action == "place":

self.execute_place(params.get("location"))

elif action == "scan":

self.execute_scan()

elif action == "report":

self.execute_report(params.get("message"))

def  execute_navigate(self, location):

if location in  self.known_locations:

coords = self.known_locations[location]

goal = PoseStamped()

goal.header.frame_id = "map"

goal.header.stamp = self.get_clock().now().to_msg()

goal.pose.position.x = coords["x"]

goal.pose.position.y = coords["y"]

goal.pose.orientation.w = 1.0

self.nav_goal_pub.publish(goal)

self.get_logger().info(f'Navigating to {location}')

else:

self.get_logger().warn(f'Unknown location: {location}')

def  execute_grasp(self, object_name):

msg = String()

msg.data = f'grasp:{object_name}'

self.action_pub.publish(msg)

self.get_logger().info(f'Grasping {object_name}')

def  execute_place(self, location):

msg = String()

msg.data = f'place:{location}'

self.action_pub.publish(msg)

self.get_logger().info(f'Placing at {location}')

def  execute_scan(self):

msg = String()

msg.data = 'scan'

self.action_pub.publish(msg)

self.get_logger().info('Scanning environment')

def  execute_report(self, message):

self.get_logger().info(f'Report: {message}')

  

def  main(args=None):

rclpy.init(args=args)

node = TaskPlannerNode()

rclpy.spin(node)

node.destroy_node()

rclpy.shutdown()

  

if  __name__ == '__main__':

main()

```

  

### **Testing the Planner**

  

```bash

# Terminal 1: Task planner

ros2  run  simple_robot  task_planner_node

  

# Terminal 2: Publish test command

ros2  topic  pub  /voice_command  std_msgs/msg/String  "data: 'Go to the kitchen and scan for objects'"

  

# Check logs for generated plan

```

  

Expected output:

```json

{

"plan": [

{"action": "navigate", "parameters": {"location": "kitchen"}},

{"action": "scan", "parameters": {}}

]

}

```

  

### **Adding Error Handling and Replanning**

  

Real robots encounter failures. Add recovery:

  

```python

def  execute_plan(self, plan):

for i, step in  enumerate(plan.get("plan", [])):

success = self.execute_step(step)

if  not success:

self.get_logger().warn(f'Step {i} failed, replanning...')

# Ask LLM to replan

remaining_steps = plan["plan"][i:]

new_plan = self.replan(remaining_steps)

# Execute new plan

self.execute_plan(new_plan)

break

  

def  replan(self, failed_steps):

prompt = f"""The robot attempted these actions but failed:

{json.dumps(failed_steps, indent=2)}

  

Generate an alternative plan to achieve the same goal.

Respond with JSON only."""

  

response = openai.ChatCompletion.create(

model="gpt-4",

messages=[{"role": "user", "content": prompt}],

temperature=0.5

)

return json.loads(response.choices[0].message.content)

```

  

### **Adding Memory and Context**

  

Maintain conversation history for context-aware planning:

  

```python

class  TaskPlannerNode(Node):

def  __init__(self):

super().__init__('task_planner_node')

# ... previous init ...

self.conversation_history = []

self.world_state = {

"robot_location": "home",

"holding_object": None,

"known_objects": []

}

def  generate_plan(self, command):

# Add context to prompt

context = f"""

Current world state:

{json.dumps(self.world_state, indent=2)}

  

Conversation history:

{json.dumps(self.conversation_history[-5:], indent=2)}

  

User command: "{command}"

"""

# Add to history

self.conversation_history.append({

"role": "user",

"content": command

})

# Generate plan with context

# ... rest of planning logic ...

```

  

### **Using Open-Source Models**

  

For privacy or cost reasons, use local models:

  

```python

from transformers import AutoModelForCausalLM, AutoTokenizer

  

class  LocalLLMPlanner:

def  __init__(self):

model_name = "mistralai/Mistral-7B-Instruct-v0.2"

self.tokenizer = AutoTokenizer.from_pretrained(model_name)

self.model = AutoModelForCausalLM.from_pretrained(

model_name,

device_map="auto",

load_in_4bit=True  # Quantization for efficiency

)

def  generate_plan(self, prompt):

inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")

outputs = self.model.generate(

**inputs,

max_new_tokens=512,

temperature=0.3

)

return  self.tokenizer.decode(outputs[0], skip_special_tokens=True)